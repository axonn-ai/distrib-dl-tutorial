srun -n 4 python train_axonn_intra_layer.py --num-layers 4 --hidden-size 2048 --data-dir /home/sathwik7/scratch.bhatele-lab/tutorial-venv --batch-size 32 --lr 0.0001 --image-size 64 --G-intra-r 2 --G-intra-c 2 --G-data 1 --micro-batch-size 4 --checkpoint-activations
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
Rank 0 : initialized AxoNN
Rank 0 : Model Params = 0.167944232 B
Rank 0 : Start Training with AxoNN's Intra-Layer Parallelism
/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:579: UserWarning: For MPI backend, world_size (4) and rank (0) are ignored since they are assigned by the MPI runtime.
  warnings.warn(
/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:579: UserWarning: For MPI backend, world_size (4) and rank (2) are ignored since they are assigned by the MPI runtime.
  warnings.warn(
/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:579: UserWarning: For MPI backend, world_size (4) and rank (3) are ignored since they are assigned by the MPI runtime.
  warnings.warn(
/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:579: UserWarning: For MPI backend, world_size (4) and rank (1) are ignored since they are assigned by the MPI runtime.
  warnings.warn(
Traceback (most recent call last):
  File "train_axonn_intra_layer.py", line 91, in <module>
    iter_loss = ax.run_batch(img, label)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 714, in run_batch
    _forward_pass(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 479, in _forward_pass
    output_activation = model(input_activation)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_3_intra_layer_parallelism/../model/fc_net_tensor_parallel.py", line 22, in forward
    x = layer(x)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_3_intra_layer_parallelism/../model/fc_net_tensor_parallel.py", line 46, in forward
    h = self.linear_1(x)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 222, in forward
    x = AsyncLinear.apply(
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py", line 110, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 72, in forward
    output = input_.matmul(weight.t())
RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x512 and 1024x4096)
Traceback (most recent call last):
  File "train_axonn_intra_layer.py", line 91, in <module>
    iter_loss = ax.run_batch(img, label)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 714, in run_batch
    _forward_pass(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 479, in _forward_pass
    output_activation = model(input_activation)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_3_intra_layer_parallelism/../model/fc_net_tensor_parallel.py", line 22, in forward
    x = layer(x)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_3_intra_layer_parallelism/../model/fc_net_tensor_parallel.py", line 46, in forward
    h = self.linear_1(x)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 222, in forward
    x = AsyncLinear.apply(
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py", line 110, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 72, in forward
    output = input_.matmul(weight.t())
RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x512 and 1024x4096)
Traceback (most recent call last):
  File "train_axonn_intra_layer.py", line 91, in <module>
    iter_loss = ax.run_batch(img, label)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 714, in run_batch
    _forward_pass(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 479, in _forward_pass
    output_activation = model(input_activation)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_3_intra_layer_parallelism/../model/fc_net_tensor_parallel.py", line 22, in forward
    x = layer(x)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_3_intra_layer_parallelism/../model/fc_net_tensor_parallel.py", line 46, in forward
    h = self.linear_1(x)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 222, in forward
    x = AsyncLinear.apply(
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py", line 110, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 72, in forward
    output = input_.matmul(weight.t())
RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x512 and 1024x4096)
Traceback (most recent call last):
  File "train_axonn_intra_layer.py", line 91, in <module>
    iter_loss = ax.run_batch(img, label)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 714, in run_batch
    _forward_pass(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 479, in _forward_pass
    output_activation = model(input_activation)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_3_intra_layer_parallelism/../model/fc_net_tensor_parallel.py", line 22, in forward
    x = layer(x)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_3_intra_layer_parallelism/../model/fc_net_tensor_parallel.py", line 46, in forward
    h = self.linear_1(x)
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 222, in forward
    x = AsyncLinear.apply(
  File "/cvmfs/hpcsw.umd.edu/spack-software/2022.06.15/linux-rhel8-zen2/gcc-9.4.0/py-torch-1.11.0-xsbh24pmd2hwdp7tikss4digwup6bemt/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py", line 110, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 72, in forward
    output = input_.matmul(weight.t())
RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x512 and 1024x4096)
srun: error: compute-a8-19: tasks 0-3: Exited with exit code 1
