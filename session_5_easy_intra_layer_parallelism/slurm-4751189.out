srun -n 4 python train.py --num-layers 4 --hidden-size 2048 --data-dir /home/sathwik7/scratch.bhatele-lab/tutorial-venv --batch-size 32 --lr 0.0001 --image-size 64 --G-intra-r 2 --G-intra-c 2 --G-data 1 --micro-batch-size 4 --checkpoint-activations
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
Rank 0 : initialized AxoNN
Rank 0 : Model Params = 0.168009768 B
Rank 0 : Start Training with AxoNN's Intra-Layer Parallelism
Traceback (most recent call last):
  File "train.py", line 89, in <module>
    iter_loss = ax.run_batch(img, label)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 729, in run_batch
    _forward_pass(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 490, in _forward_pass
    output_activation = model(input_activation)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_5_easy_intra_layer_parallelism/../model/fc_net_easy_tensor_parallel.py", line 17, in forward
    x = layer(x)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_5_easy_intra_layer_parallelism/../model/fc_net_easy_tensor_parallel.py", line 33, in forward
    h = self.linear_1(x)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 307, in forward
    bias = Gather.apply(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/communication.py", line 156, in forward
    return _gather(input_, dim=dim, process_group=process_group)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/communication.py", line 52, in _gather
    dist.all_gather(tensor_list, input_, group=process_group)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2282, in all_gather
    work.wait()
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
Traceback (most recent call last):
  File "train.py", line 89, in <module>
    iter_loss = ax.run_batch(img, label)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 729, in run_batch
    _forward_pass(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 490, in _forward_pass
    output_activation = model(input_activation)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_5_easy_intra_layer_parallelism/../model/fc_net_easy_tensor_parallel.py", line 17, in forward
    x = layer(x)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_5_easy_intra_layer_parallelism/../model/fc_net_easy_tensor_parallel.py", line 33, in forward
    h = self.linear_1(x)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 307, in forward
    bias = Gather.apply(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/communication.py", line 156, in forward
    return _gather(input_, dim=dim, process_group=process_group)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/communication.py", line 52, in _gather
    dist.all_gather(tensor_list, input_, group=process_group)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2282, in all_gather
    work.wait()
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
Traceback (most recent call last):
  File "train.py", line 89, in <module>
    iter_loss = ax.run_batch(img, label)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 729, in run_batch
    _forward_pass(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 490, in _forward_pass
    output_activation = model(input_activation)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_5_easy_intra_layer_parallelism/../model/fc_net_easy_tensor_parallel.py", line 17, in forward
    x = layer(x)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_5_easy_intra_layer_parallelism/../model/fc_net_easy_tensor_parallel.py", line 33, in forward
    h = self.linear_1(x)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 307, in forward
    bias = Gather.apply(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/communication.py", line 156, in forward
    return _gather(input_, dim=dim, process_group=process_group)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/communication.py", line 52, in _gather
    dist.all_gather(tensor_list, input_, group=process_group)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2282, in all_gather
    work.wait()
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
Traceback (most recent call last):
  File "train.py", line 89, in <module>
    iter_loss = ax.run_batch(img, label)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 729, in run_batch
    _forward_pass(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/axonn.py", line 490, in _forward_pass
    output_activation = model(input_activation)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_5_easy_intra_layer_parallelism/../model/fc_net_easy_tensor_parallel.py", line 17, in forward
    x = layer(x)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/distrib-dl-tutorial/session_5_easy_intra_layer_parallelism/../model/fc_net_easy_tensor_parallel.py", line 33, in forward
    h = self.linear_1(x)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/fully_connected.py", line 307, in forward
    bias = Gather.apply(
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/communication.py", line 156, in forward
    return _gather(input_, dim=dim, process_group=process_group)
  File "/scratch/zt1/project/bhatele-lab/user/sathwik7/tutorial-venv/axonn/axonn/intra_layer/communication.py", line 52, in _gather
    dist.all_gather(tensor_list, input_, group=process_group)
  File "/home/sathwik7/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2282, in all_gather
    work.wait()
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
srun: error: compute-b7-46: tasks 0-3: Exited with exit code 1
